{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eb52e9",
   "metadata": {},
   "source": [
    "#####  I used a database of an audiobook app from customers that made a purchase at least once. The Algorithm takes it as input and predict whether the customers will buy again from the audiobook company. With that, the company could target customers that are more likely to come back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "0f92ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9984e1",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "6504800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "# Removing the first column 'ids' and last column targets to the main data\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "#  Getting the last column targets\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33e98a",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "139cf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of '1' in the targets column\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Defining the number of '0' in the targets column and appending the difference from '1' to be removed\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter +=1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(int(i))\n",
    "# Removing '0' from inputs and targets, and balancing with the number of '1'             \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b196b1e",
   "metadata": {},
   "source": [
    "### Standardize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "43ee6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the inputs\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50192c4",
   "metadata": {},
   "source": [
    "### Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d4bf148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling a variable\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Mapping the shuffled variable on inputs and targets also shuffling them\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58298005",
   "metadata": {},
   "source": [
    "### Split into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d85cb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796.0 3579 0.5018161497625034\n",
      "232.0 447 0.5190156599552572\n",
      "209.0 447 0.46756152125279643\n"
     ]
    }
   ],
   "source": [
    "# Creating the proportion of train, validation and targets\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = int(0.1*samples_count)\n",
    "\n",
    "# Creating the train input and targets section\n",
    "train_inputs = shuffled_inputs[: train_samples_count]\n",
    "train_targets = shuffled_targets[: train_samples_count]\n",
    "\n",
    "# Creating the validation input and targets section\n",
    "validation_inputs = shuffled_inputs[train_samples_count : train_samples_count + validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count : train_samples_count + validation_samples_count]\n",
    "\n",
    "# Creating the test input and targets section\n",
    "test_inputs = shuffled_inputs[train_samples_count + validation_samples_count :]\n",
    "test_targets = shuffled_targets[train_samples_count + validation_samples_count :]\n",
    "\n",
    "# printing the targets to analise how disperse they are\n",
    "print(np.sum(train_targets),train_samples_count,np.sum(train_targets)/train_samples_count)\n",
    "print(np.sum(validation_targets),validation_samples_count,np.sum(validation_targets)/validation_samples_count)\n",
    "print(np.sum(test_targets),test_samples_count,np.sum(test_targets)/test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a1885",
   "metadata": {},
   "source": [
    "### Save in .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "0634ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train, validation and test in npz files\n",
    "np.savez('Audiobooks_data_train',inputs=train_inputs,targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs,targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628f0f3",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "223f4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train data and creating variables in float in int\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "train_input = npz['inputs'].astype(np.float64)\n",
    "train_target = npz['targets'].astype(np.int32)\n",
    "\n",
    "# Loading validation data and creating variables in float in int\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_input = npz['inputs'].astype(np.float64)\n",
    "validation_target = npz['targets'].astype(np.int32)\n",
    "\n",
    "# Loading test data and creating variables in float in int\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_input = npz['inputs'].astype(np.float64)\n",
    "test_target = npz['targets'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b9d53",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "48ed5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring variables for width of the inputs, outputs and hidden layers.\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# Defining the actual model\n",
    "model = tf.keras.Sequential([\n",
    "                          # Biulding the neural network. It takes the inputs provided to the model and calculates\n",
    "                          # the inputs and weights dot product and adds the bias. Also applies the activation function.\n",
    "                          tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "                          # Creating a second hidden layer with the activation function.\n",
    "                          tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "                          # Creating the output layer. Because this program is a classifier, activation function of the\n",
    "                          # output layer must transforme the values into probabilities, for that I used 'SoftMax'\n",
    "                          tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5915f",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b6093b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the optimizer and loss function, for optimazer I defined 'Adam'.\n",
    "# Loss has to be for classifiers, then I defined 'sparse_categorical_crossentropy' -\n",
    "# because I did not one-hot encoded the targets\n",
    "# Including metrics to calculate throughout the training and testing processes.\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c7d13",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "04274fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - loss: 0.5750 - accuracy: 0.6806 - val_loss: 0.4916 - val_accuracy: 0.7763 - 1s/epoch - 29ms/step\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4636 - accuracy: 0.7656 - val_loss: 0.4199 - val_accuracy: 0.7919 - 98ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.4116 - accuracy: 0.7837 - val_loss: 0.3885 - val_accuracy: 0.8076 - 103ms/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3854 - accuracy: 0.7885 - val_loss: 0.3722 - val_accuracy: 0.8076 - 99ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3714 - accuracy: 0.7974 - val_loss: 0.3685 - val_accuracy: 0.8031 - 126ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3647 - accuracy: 0.7974 - val_loss: 0.3559 - val_accuracy: 0.8121 - 125ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3543 - accuracy: 0.8089 - val_loss: 0.3593 - val_accuracy: 0.8054 - 141ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a1de3d2850>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a batch size\n",
    "batch_size = 100\n",
    "# Creating a variable to store the number of epoch\n",
    "max_epochs = 100\n",
    "\n",
    "# Defining a early stopping in the algorithm to stop before the overfitting, setting patience to 2\n",
    "# to avoid an excessive restriction\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=1)\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(train_input,\n",
    "          train_targets,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs,validation_targets),\n",
    "          verbose=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "6a238ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3849 - accuracy: 0.7902\n"
     ]
    }
   ],
   "source": [
    "# Testing the model, it returns the loss and metrics for the model in 'test_mode'\n",
    "test_loss, test_accuracy = model.evaluate(test_input,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
